"""
Trade Mini - ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
"""

import asyncio
import logging
import multiprocessing
import signal
import sys
import threading
import time
from collections import defaultdict, deque
from datetime import datetime, timedelta
from typing import Any, Dict

# ãƒ­ã‚°è¨­å®š
from loguru import logger as loguru_logger

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ­ã‚¬ãƒ¼
logger = loguru_logger

from bybit_client import BybitClient

# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from config import Config
from data_manager import DataManager
from mexc_client import MEXCClient, TickData
from position_manager import PositionManager
from questdb_client import QuestDBClient, QuestDBTradeRecordManager
from strategy import SignalType, TradingStrategy
from symbol_mapper import SymbolMapper


class TradeMini:
    """Trade Mini ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""

    def __init__(self, config_path: str = "config.yml"):
        """
        åˆæœŸåŒ–

        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        # è¨­å®šèª­ã¿è¾¼ã¿
        self.config = Config(config_path)

        # ãƒ­ã‚°è¨­å®š
        self._setup_logging()

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.mexc_client = None
        self.bybit_client = None
        self.symbol_mapper = None
        self.data_manager = None
        self.strategy = None
        self.position_manager = None
        self.questdb_client = None
        self.trade_record_manager = None

        # å®Ÿè¡Œåˆ¶å¾¡
        self.running = False
        self.shutdown_event = threading.Event()

        # çµ±è¨ˆ
        self.stats = {
            "start_time": datetime.now(),
            "ticks_processed": 0,
            "signals_generated": 0,
            "trades_executed": 0,
            "uptime": 0.0,
        }

        # å¤‰å‹•ç‡çµ±è¨ˆï¼ˆéåŒæœŸåé›†ï¼‰
        self.price_changes = {
            "max_change": 0.0,
            "max_change_symbol": "",
            "max_change_direction": "",
            "last_report_time": datetime.now(),
            "changes_since_last_report": 0,
        }

        # çµ±è¨ˆè¡¨ç¤ºã‚¿ã‚¤ãƒãƒ¼
        self.stats_timer = None

        # ğŸ›¡ï¸ çœŸã®ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹åˆ†é›¢è¨­è¨ˆ
        self.data_queue = multiprocessing.Queue(maxsize=10)  # ãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡ã‚­ãƒ¥ãƒ¼
        self.processing_active = multiprocessing.Value(
            "b", True
        )  # ãƒ—ãƒ­ã‚»ã‚¹é–“å…±æœ‰ãƒ•ãƒ©ã‚°
        self.worker_heartbeat = multiprocessing.Value(
            "d", time.time()
        )  # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆ
        self.data_processor = None  # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ—ãƒ­ã‚»ã‚¹

        # ğŸ“Š ä¾¡æ ¼å±¥æ­´ç®¡ç†ï¼ˆ10ç§’å‰æ¯”è¼ƒç”¨ï¼‰ - symbol -> deque([(timestamp_sec, price), ...])
        self.price_history = defaultdict(lambda: deque(maxlen=15))  # ç´„15ç§’åˆ†ã®ãƒãƒƒãƒ•ã‚¡

        # ğŸ“ˆ çµ±è¨ˆã‚«ã‚¦ãƒ³ã‚¿ï¼ˆWebSocketå—ä¿¡ã¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã§åˆ†é›¢ï¼‰
        self.reception_stats = {"batches_received": 0, "tickers_received": 0}
        self.processing_stats = {"batches_processed": 0, "tickers_processed": 0}

        logger.info("Trade Mini initialized")

    def _setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        # æ—¢å­˜ã®ãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’å‰Šé™¤
        loguru_logger.remove()

        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        loguru_logger.add(
            sys.stderr,
            level=self.config.log_level,
            format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
            "<level>{level: <8}</level> | "
            "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - "
            "<level>{message}</level>",
        )

        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        loguru_logger.add(
            self.config.log_file,
            level=self.config.log_level,
            rotation=f"{self.config.get('logging.max_size_mb', 10)} MB",
            retention=self.config.get("logging.backup_count", 5),
            encoding="utf-8",
        )

        # æ¨™æº–loggingãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’loguru ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
        logging.basicConfig(handlers=[], level=logging.DEBUG)
        logging.getLogger().handlers.clear()

        class InterceptHandler(logging.Handler):
            def emit(self, record):
                try:
                    level = loguru_logger.level(record.levelname).name
                except ValueError:
                    level = record.levelno

                frame, depth = logging.currentframe(), 2
                while frame.f_code.co_filename == logging.__file__:
                    frame = frame.f_back
                    depth += 1

                loguru_logger.opt(depth=depth, exception=record.exc_info).log(
                    level, record.getMessage()
                )

        logging.getLogger().addHandler(InterceptHandler())

    async def initialize(self):
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–"""
        logger.info("Initializing components...")

        try:
            # MEXC ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆãƒ†ã‚£ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾—ç”¨ï¼‰
            self.mexc_client = MEXCClient(self.config)
            # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã‚­ãƒ¥ãƒ¼ã‚’è¨­å®š
            self.mexc_client.set_data_queue(self.data_queue)
            logger.info("MEXC client created and data queue configured")

            # Bybit ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆæ³¨æ–‡ãƒ»æ±ºæ¸ˆç”¨ï¼‰
            self.bybit_client = BybitClient(
                self.config.bybit_api_key,
                self.config.bybit_api_secret,
                self.config.bybit_environment,
                self.config.bybit_api_url,
            )
            logger.info("Bybit client created")

            # éŠ˜æŸ„ãƒãƒƒãƒ”ãƒ³ã‚°ç®¡ç†
            self.symbol_mapper = SymbolMapper(self.bybit_client)
            logger.info("Symbol mapper created")

            # ãƒ‡ãƒ¼ã‚¿ç®¡ç†
            self.data_manager = DataManager(self.config)
            logger.info("Data manager created")

            # å–å¼•æˆ¦ç•¥
            self.strategy = TradingStrategy(self.config, self.data_manager)
            logger.info("Trading strategy created")

            # ãƒã‚¸ã‚·ãƒ§ãƒ³ç®¡ç†
            self.position_manager = PositionManager(
                self.config, self.mexc_client, self.bybit_client, self.symbol_mapper
            )
            logger.info("Position manager created")

            # QuestDB ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
            self.questdb_client = QuestDBClient(self.config)
            self.trade_record_manager = QuestDBTradeRecordManager(self.questdb_client)
            logger.info("QuestDB client created")

            # MEXC WebSocket æ¥ç¶š
            if not await self.mexc_client.start():
                raise Exception("Failed to connect to MEXC WebSocket")

            # ãƒ†ã‚£ãƒƒã‚«ãƒ¼ãƒãƒƒãƒã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³B'ï¼‰
            self.mexc_client.set_batch_callback(self._on_ticker_batch_received)

            # å…¨éŠ˜æŸ„è³¼èª­
            if not await self.mexc_client.subscribe_all_tickers():
                raise Exception("Failed to subscribe to all tickers")

            # çµ±è¨ˆè¡¨ç¤ºã‚¿ã‚¤ãƒãƒ¼é–‹å§‹
            self._start_stats_timer()

            # ğŸš€ çœŸã®ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼é–‹å§‹ï¼ˆGILå®Œå…¨å›é¿ï¼‰
            self._start_multiprocess_data_worker()

            logger.info("All components initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize components: {e}")
            await self.shutdown()
            raise

    def _on_ticker_batch_received(self, tickers: list):
        """WebSocketå—ä¿¡ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆçœŸã®ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹åˆ†é›¢ï¼‰"""
        try:
            # ğŸš€ å—ä¿¡è¨¼æ˜ã®ã¿ï¼ˆæ¥µé™ã®è»½é‡åŒ– < 0.001msï¼‰
            self.reception_stats["batches_received"] += 1
            current_time = datetime.now().strftime("%H:%M:%S.%f")[:-3]

            # ğŸ“¨ å—ä¿¡è¨¼æ˜ãƒ­ã‚°ã®ã¿
            logger.info(
                f"ğŸ”¥ [{current_time}] WebSocket ALIVE! Batch #{self.reception_stats['batches_received']}: {len(tickers)} tickers â†’ Multi-Process Queue"
            )

            # ğŸ¯ ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã‚­ãƒ¥ãƒ¼ã«ç¬é–“æŠ•å…¥ï¼ˆãƒãƒ³ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
            try:
                # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾é€ä¿¡ï¼ˆå¤‰æ›å‡¦ç†ãªã—ï¼‰
                self.data_queue.put_nowait(
                    {
                        "tickers": tickers,
                        "timestamp": time.time(),
                        "batch_id": self.reception_stats["batches_received"],
                    }
                )
            except:
                # ã‚­ãƒ¥ãƒ¼ãŒæº€æ¯ã§ã‚‚å—ä¿¡ã¯ç¶™ç¶šï¼ˆãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚ˆã‚Šå—ä¿¡ã‚’å„ªå…ˆï¼‰
                logger.debug(
                    f"Multi-process queue full, skipping batch #{self.reception_stats['batches_received']}"
                )

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚WebSocketå—ä¿¡ã¯çµ¶å¯¾ã«åœæ­¢ã—ãªã„
            logger.error(f"Error in reception callback: {e}")

    def _start_multiprocess_data_worker(self):
        """ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’é–‹å§‹"""
        logger.info("ğŸš€ Starting multi-process data worker (true process separation)")

        # ç‹¬ç«‹ãƒ—ãƒ­ã‚»ã‚¹ã§ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å®Ÿè¡Œ
        self.data_processor = multiprocessing.Process(
            target=self._multiprocess_data_worker,
            args=(self.data_queue, self.processing_active, self.worker_heartbeat),
            daemon=True,
        )
        self.data_processor.start()
        logger.info(
            f"âœ… Multi-process data worker started with PID: {self.data_processor.pid}"
        )

    @staticmethod
    def _multiprocess_data_worker(
        data_queue: multiprocessing.Queue,
        processing_active: multiprocessing.Value,
        worker_heartbeat: multiprocessing.Value,
    ):
        """ç‹¬ç«‹ãƒ—ãƒ­ã‚»ã‚¹ã§ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆGILå®Œå…¨å›é¿ï¼‰"""
        import time
        from datetime import datetime

        # ãƒ—ãƒ­ã‚»ã‚¹ç‹¬ç«‹ãƒ­ã‚°è¨­å®š
        from loguru import logger

        logger.add("multiprocess_worker.log", rotation="1 MB")

        logger.info(
            f"ğŸ”„ Multi-process data worker started in PID: {multiprocessing.current_process().pid}"
        )

        last_heartbeat = time.time()

        while processing_active.value:
            try:
                # ğŸ©¸ ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆæ›´æ–°ï¼ˆ5ç§’æ¯ï¼‰
                current_time = time.time()
                if current_time - last_heartbeat >= 5.0:
                    worker_heartbeat.value = current_time
                    last_heartbeat = current_time
                    logger.debug(
                        f"ğŸ’“ Worker heartbeat: {datetime.fromtimestamp(current_time).strftime('%H:%M:%S')}"
                    )

                # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                try:
                    batch_data = data_queue.get(timeout=1.0)
                except:
                    continue  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯æ¬¡ã®å¾ªç’°ã¸

                # MEXCã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œ
                if "raw_data" in batch_data:
                    # MEXCã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®æ–°ã—ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                    raw_data = batch_data["raw_data"]
                    tickers = raw_data["data"]
                    batch_timestamp = batch_data["rx_time"]
                    batch_id = batch_data["message_count"]
                else:
                    # æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆäº’æ›æ€§ç¶­æŒï¼‰
                    tickers = batch_data["tickers"]
                    batch_timestamp = batch_data["timestamp"]
                    batch_id = batch_data["batch_id"]

                # ğŸš€ é«˜é€Ÿå‡¦ç†ï¼ˆJSONã‹ã‚‰QuestDBå½¢å¼ã¸ã®ç›´æ¥å¤‰æ›ï¼‰
                TradeMini._process_batch_lightning_fast(
                    tickers, batch_timestamp, batch_id
                )

                # å‡¦ç†å¾Œã«ã‚‚ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆæ›´æ–°
                worker_heartbeat.value = time.time()

            except Exception as e:
                logger.error(f"Error in multi-process data worker: {e}")
                time.sleep(0.1)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯çŸ­æ™‚é–“å¾…æ©Ÿ

        logger.info("Multi-process data worker shutdown completed")

    def _check_multiprocess_health(self):
        """ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        try:
            current_time = time.time()

            # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®ç”Ÿå­˜ç¢ºèª
            if self.data_processor and not self.data_processor.is_alive():
                logger.error(
                    "ğŸš¨ Multi-process data worker is dead! Attempting restart..."
                )
                self._restart_multiprocess_worker()
                return

            # ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
            last_heartbeat = self.worker_heartbeat.value
            heartbeat_age = current_time - last_heartbeat

            if heartbeat_age > 30.0:  # 30ç§’ä»¥ä¸Šãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãŒãªã„
                logger.warning(f"âš ï¸ Worker heartbeat stale: {heartbeat_age:.1f}s ago")
                if heartbeat_age > 60.0:  # 1åˆ†ä»¥ä¸Šãªã‚‰å¼·åˆ¶å†èµ·å‹•
                    logger.error("ğŸš¨ Worker heartbeat timeout! Restarting worker...")
                    self._restart_multiprocess_worker()
                    return

            # ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºç›£è¦–
            queue_size = self.data_queue.qsize()
            if queue_size >= 8:  # ã‚­ãƒ¥ãƒ¼ãŒè©°ã¾ã£ã¦ã„ã‚‹
                logger.warning(f"âš ï¸ Data queue congestion: {queue_size}/10 items")

            # æ­£å¸¸æ™‚ã®ãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ
            worker_pid = self.data_processor.pid if self.data_processor else "None"
            logger.debug(
                f"ğŸ’ª Health check OK - Worker PID: {worker_pid}, Queue: {queue_size}/10, Heartbeat: {heartbeat_age:.1f}s ago"
            )

        except Exception as e:
            logger.error(f"Error in health check: {e}")

    def _restart_multiprocess_worker(self):
        """ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’å†èµ·å‹•"""
        try:
            logger.info("ğŸ”„ Restarting multi-process data worker...")

            # å¤ã„ãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢
            if self.data_processor:
                self.processing_active.value = False
                self.data_processor.terminate()
                self.data_processor.join(timeout=5)
                if self.data_processor.is_alive():
                    logger.warning("Force killing stuck worker process")
                    self.data_processor.kill()

            # æ–°ã—ã„ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹
            self.processing_active.value = True
            self.worker_heartbeat.value = time.time()
            self._start_multiprocess_data_worker()

            logger.info("âœ… Multi-process worker restart completed")

        except Exception as e:
            logger.error(f"Failed to restart multi-process worker: {e}")

    @staticmethod
    def _process_batch_lightning_fast(
        tickers: list, batch_timestamp: float, batch_id: int
    ):
        """è¶…é«˜é€Ÿãƒãƒƒãƒå‡¦ç†ï¼ˆJSONã‹ã‚‰ç›´æ¥QuestDBå½¢å¼ã«å¤‰æ›ï¼‰"""
        import socket
        import time
        from datetime import datetime

        start_time = time.time()
        processed_count = 0
        questdb_lines = []

        try:
            # ğŸš€ JSONã‹ã‚‰ç›´æ¥QuestDB ILPå½¢å¼ã«å¤‰æ›ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãªã—ï¼‰
            batch_ts_ns = int(batch_timestamp * 1_000_000_000)

            for ticker_data in tickers:
                if not isinstance(ticker_data, dict):
                    continue

                symbol = ticker_data.get("symbol", "")
                price = ticker_data.get("lastPrice")
                volume = ticker_data.get("volume24", "0")

                if symbol and price:
                    try:
                        price_f = float(price)
                        volume_f = float(volume)

                        # QuestDB ILPå½¢å¼ã§ç›´æ¥ç”Ÿæˆ
                        line = f"tick_data,symbol={symbol} price={price_f},volume={volume_f} {batch_ts_ns}"
                        questdb_lines.append(line)
                        processed_count += 1

                    except (ValueError, TypeError):
                        continue

            # ğŸš€ QuestDBä¸€æ‹¬æ›¸ãè¾¼ã¿ï¼ˆè¶…é«˜é€Ÿå®Ÿè£…ï¼‰
            questdb_saved = 0
            if questdb_lines:
                questdb_saved = TradeMini._send_to_questdb_lightning(questdb_lines)

            duration = time.time() - start_time
            # printã‚’loguruãƒ­ã‚°ã«å¤‰æ›´
            from loguru import logger

            logger.info(
                f"âš¡ Lightning batch #{batch_id}: {processed_count}/{len(tickers)} processed, {questdb_saved} saved to QuestDB in {duration:.3f}s"
            )

        except Exception as e:
            from loguru import logger

            logger.error(f"Error in lightning processing: {e}")

    @staticmethod
    def _send_to_questdb_lightning(ilp_lines: list) -> int:
        """QuestDBã«è¶…é«˜é€Ÿã§ä¸€æ‹¬é€ä¿¡ï¼ˆãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ç”¨ï¼‰"""
        try:
            import socket  # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å†…ã§æ˜ç¤ºçš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

            # QuestDB ILPæ¥ç¶š
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(5.0)
            sock.connect(("questdb", 9009))

            # å…¨è¡Œã‚’ä¸€æ‹¬é€ä¿¡
            ilp_data = "\n".join(ilp_lines) + "\n"
            sock.sendall(ilp_data.encode("utf-8"))
            sock.close()

            from loguru import logger

            logger.debug(f"âœ… QuestDB ILP: {len(ilp_lines)} records sent successfully")
            return len(ilp_lines)

        except Exception as e:
            from loguru import logger

            logger.warning(f"QuestDB write error: {e}")
            return 0

    async def _process_single_batch_efficiently(
        self, tickers: list, batch_timestamp: float, batch_id: int
    ):
        """1ã¤ã®ã‚¿ã‚¹ã‚¯ã§å…¨éŠ˜æŸ„ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ï¼ˆGILåˆ¶ç´„è€ƒæ…®ï¼‰"""
        try:
            start_time = time.time()
            batch_ts_sec = int(batch_timestamp)
            trading_exchange = self.config.get("trading.exchange", "bybit")

            # å‡¦ç†çµ±è¨ˆæ›´æ–°
            self.processing_stats["batches_processed"] += 1
            self.processing_stats["tickers_processed"] += len(tickers)

            logger.info(
                f"ğŸ”„ Processing batch #{batch_id}: {len(tickers)} tickers (å…¨éŠ˜æŸ„åˆ†æ - ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ©Ÿä¼šã‚’é€ƒã•ãªã„)"
            )

            # ğŸ“Š åŠ¹ç‡çš„ãªä¸€æ‹¬å‡¦ç†ï¼ˆå…¨éŠ˜æŸ„å¯¾å¿œ - è»½é‡åŒ–ï¼‰
            signals_count = 0
            significant_changes = 0
            processed_count = 0
            tradeable_count = 0

            # ğŸš€ QuestDBä¸€æ‹¬æ›¸ãè¾¼ã¿ç”¨ã®ãƒªã‚¹ãƒˆ
            batch_ticks_for_questdb = []

            # å…¨éŠ˜æŸ„ã‚’é †æ¬¡å‡¦ç†ï¼ˆ1ã¤ã®ã‚¿ã‚¹ã‚¯å†…ã§å®Œçµ - è»½é‡ç‰ˆï¼‰
            for ticker_data in tickers:
                # ğŸš€ å‡¦ç†æ•°ã®åˆ¶é™ã§æ—©æœŸçµ‚äº†ï¼ˆWebSocketå—ä¿¡ã‚’ä¿è­·ï¼‰
                if processed_count >= 500:  # æœ€å¤§500éŠ˜æŸ„ã¾ã§å‡¦ç†
                    break
                if not isinstance(ticker_data, dict):
                    continue

                symbol = ticker_data.get("symbol", "")
                price = float(ticker_data.get("lastPrice", 0))

                if not symbol or price <= 0:
                    continue

                # ğŸ“ˆ ä¾¡æ ¼å±¥æ­´æ›´æ–°ï¼ˆé«˜é€Ÿï¼‰
                self.price_history[symbol].append((batch_ts_sec, price))
                price_change_percent = self._update_price_history_and_get_change(
                    symbol, price, batch_ts_sec
                )

                # TickDataä½œæˆ
                tick = TickData(
                    symbol=symbol,
                    price=price,
                    timestamp=datetime.now(),
                    volume=float(ticker_data.get("volume24", 0)),
                )

                # ãƒ‡ãƒ¼ã‚¿ç®¡ç†
                self.data_manager.add_tick(tick)

                # ğŸ¯ æˆ¦ç•¥åˆ†æï¼ˆå…¨éŠ˜æŸ„å¯¾å¿œ - ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ©Ÿä¼šã‚’é€ƒã•ãªã„ï¼‰
                if trading_exchange == "bybit":
                    if self.symbol_mapper.is_tradeable_on_bybit(symbol):
                        tradeable_count += 1

                        # æˆ¦ç•¥åˆ†æã‚’è»½é‡åŒ–ï¼ˆå‡¦ç†æ™‚é–“ã‚’çŸ­ç¸®ï¼‰
                        if tradeable_count <= 50:  # æœ€åˆã®50éŠ˜æŸ„ã®ã¿è©³ç´°åˆ†æ
                            signal = self.strategy.analyze_tick(tick)

                            if signal and signal.signal_type != SignalType.NONE:
                                signals_count += 1
                                logger.info(
                                    f"ğŸš¨ SIGNAL: {signal.symbol} {signal.signal_type.value} @ {signal.price:.6f}"
                                )

                                # ğŸš€ ã‚·ã‚°ãƒŠãƒ«å‡¦ç†ã‚’éåŒæœŸã§å®Ÿè¡Œï¼ˆWebSocketå—ä¿¡ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
                                asyncio.create_task(self._process_signal(signal))

                # ğŸ“Š çµ±è¨ˆåé›†ï¼ˆå…¨éŠ˜æŸ„ï¼‰
                if abs(price_change_percent) > 1.0:
                    significant_changes += 1

                # ğŸ’¾ QuestDBä¿å­˜ç”¨ãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆä¸€æ‹¬æ›¸ãè¾¼ã¿ç”¨ï¼‰
                if (
                    processed_count < 100 or abs(price_change_percent) > 1.0
                ):  # é‡è¦ãªéŠ˜æŸ„ã®ã¿ä¿å­˜
                    batch_ticks_for_questdb.append(tick)

                processed_count += 1

                # ğŸš€ å®šæœŸçš„ã«ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’è­²ã‚‹ï¼ˆWebSocketå—ä¿¡ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
                if processed_count % 25 == 0:
                    await asyncio.sleep(0.001)  # 1mså¾…æ©Ÿã§ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’è­²ã‚‹

            # ğŸš€ QuestDBä¸€æ‹¬æ›¸ãè¾¼ã¿ï¼ˆå¤§å¹…ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
            if batch_ticks_for_questdb:
                try:
                    self.questdb_client.save_batch_tick_data(batch_ticks_for_questdb)
                    logger.info(
                        f"ğŸ’¾ QuestDB batch write: {len(batch_ticks_for_questdb)} ticks saved efficiently"
                    )
                except Exception as e:
                    logger.error(f"Error in QuestDB batch write: {e}")

            # â±ï¸ å‡¦ç†æ™‚é–“è¨ˆæ¸¬
            duration = time.time() - start_time
            current_time = datetime.now().strftime("%H:%M:%S.%f")[:-3]

            logger.info(
                f"âœ… [{current_time}] Batch #{batch_id} completed: {processed_count}/{len(tickers)} processed in {duration:.3f}s, tradeable: {tradeable_count}, signals: {signals_count}, significant_changes: {significant_changes}, questdb_saved: {len(batch_ticks_for_questdb)}"
            )

        except Exception as e:
            logger.error(f"Error processing batch #{batch_id}: {e}")

    def _minimal_sync_processing(self, tickers: list):
        """åŒæœŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼ˆæœ€å°é™ã®ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã®ã¿ï¼‰"""
        try:
            logger.info(f"ğŸ”§ Minimal sync processing for {len(tickers)} tickers")
            processed_count = 0

            for ticker_data in tickers[:50]:  # æœ€åˆã®50éŠ˜æŸ„ã®ã¿å‡¦ç†ï¼ˆè² è·è»½æ¸›ï¼‰
                if not isinstance(ticker_data, dict):
                    continue

                symbol = ticker_data.get("symbol", "")
                price = float(ticker_data.get("lastPrice", 0))

                if not symbol or price <= 0:
                    continue

                # TickDataä½œæˆã¨QuestDBä¿å­˜ã®ã¿
                tick = TickData(
                    symbol=symbol,
                    price=price,
                    timestamp=datetime.now(),
                    volume=float(ticker_data.get("volume24", 0)),
                )

                # ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã«è¿½åŠ 
                self.data_manager.add_tick(tick)

                # QuestDBä¿å­˜ï¼ˆåŒæœŸï¼‰
                self.questdb_client.save_tick_data(tick)
                processed_count += 1

            logger.info(
                f"âœ… Minimal sync processing completed: {processed_count} tickers"
            )

        except Exception as e:
            logger.error(f"Error in minimal sync processing: {e}")

    async def _process_ticker_batch_controlled(self, tickers: list):
        """Semaphoreåˆ¶å¾¡ä»˜ããƒãƒƒãƒå‡¦ç†ï¼ˆWebSocketå—ä¿¡ä¿è­·ï¼‰"""
        logger.info(f"ğŸ¯ Entering batch processing control for {len(tickers)} tickers")

        # ğŸ” ç›£è¦–ï¼šå¾…æ©Ÿä¸­ã®ãƒãƒƒãƒå‡¦ç†æ•°ã‚’ãƒã‚§ãƒƒã‚¯
        waiting_batches = 2 - self.batch_processing_semaphore._value
        if waiting_batches > 0:
            logger.info(f"â³ {waiting_batches}/2 batch tasks waiting")

        logger.info(f"ğŸ”’ Acquiring batch processing semaphore...")
        async with self.batch_processing_semaphore:
            logger.info(f"âœ… Semaphore acquired, starting batch processing")
            self._batch_processing = True
            try:
                await self._process_single_batch_efficiently(
                    tickers, time.time(), self.processing_stats["batches_processed"]
                )
            finally:
                self._batch_processing = False
                logger.info(f"ğŸ”“ Batch processing completed, releasing semaphore")

    async def _background_questdb_save(self, tick: TickData):
        """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã®QuestDBä¿å­˜å‡¦ç†ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰"""
        try:
            # QuestDBä¿å­˜ï¼ˆéåŒæœŸã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã®ã¿ - ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
            self.questdb_client.save_tick_data(tick)

        except Exception as e:
            logger.error(f"Error in background QuestDB save for {tick.symbol}: {e}")

    def _update_price_history_and_get_change(
        self, symbol: str, price: float, timestamp_sec: int
    ) -> float:
        """ä¾¡æ ¼å±¥æ­´ã‚’æ›´æ–°ã—10ç§’å‰ã¨ã®å¤‰å‹•ç‡ã‚’è¨ˆç®—ï¼ˆãƒãƒƒãƒå‡¦ç†ç”¨é«˜é€Ÿç‰ˆï¼‰"""
        try:
            # ğŸš€ å±¥æ­´æ›´æ–°ï¼ˆdequeæ“ä½œã¯é«˜é€Ÿï¼‰
            self.price_history[symbol].append((timestamp_sec, price))

            # ğŸ” 10ç§’å‰ã®ä¾¡æ ¼ã‚’æ¤œç´¢ï¼ˆå¾Œã‚ã‹ã‚‰å‰ã¸åŠ¹ç‡çš„ã«æ¤œç´¢ï¼‰
            target_sec = timestamp_sec - 10
            prev_price = None

            # dequeã‚’å¾Œã‚ã‹ã‚‰æ¤œç´¢ã—ã¦ target_sec ä»¥ä¸‹ã®æœ€æ–°ä¾¡æ ¼ã‚’å–å¾—
            for ts, px in reversed(self.price_history[symbol]):
                if ts <= target_sec:
                    prev_price = px
                    break

            # ğŸ“Š å¤‰å‹•ç‡è¨ˆç®—
            if prev_price and prev_price > 0:
                change_percent = ((price - prev_price) / prev_price) * 100
                return change_percent

            return 0.0

        except Exception:
            return 0.0

    def _get_price_change_from_strategy(self, symbol: str) -> float:
        """æˆ¦ç•¥ã‹ã‚‰ä¾¡æ ¼å¤‰å‹•ç‡ã‚’å–å¾—ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰"""
        try:
            # æˆ¦ç•¥ã‹ã‚‰æœ€æ–°ã®ä¾¡æ ¼å¤‰å‹•ç‡ã‚’å–å¾—
            if hasattr(self.strategy, "get_price_change_percent"):
                return self.strategy.get_price_change_percent(symbol)
            return 0.0
        except Exception:
            return 0.0

    async def _update_price_change_stats(self, symbol: str, change_percent: float):
        """å¤‰å‹•ç‡çµ±è¨ˆã‚’éåŒæœŸã§æ›´æ–°"""
        try:
            abs_change = abs(change_percent)

            # æœ€å¤§å¤‰å‹•ç‡ã®æ›´æ–°
            if abs_change > abs(self.price_changes["max_change"]):
                self.price_changes["max_change"] = change_percent
                self.price_changes["max_change_symbol"] = symbol
                self.price_changes["max_change_direction"] = (
                    "ä¸Šæ˜‡" if change_percent > 0 else "ä¸‹è½"
                )

            self.price_changes["changes_since_last_report"] += 1

            # 15ç§’ã”ã¨ã«æœ€å¤§å¤‰å‹•ç‡ã‚’ãƒ¬ãƒãƒ¼ãƒˆï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ã«çŸ­ç¸®ï¼‰
            now = datetime.now()
            if (now - self.price_changes["last_report_time"]).total_seconds() >= 15:
                if self.price_changes["changes_since_last_report"] > 0:
                    logger.info(
                        f"ğŸ“ˆ æœ€å¤§å¤‰å‹•ç‡: {self.price_changes['max_change_symbol']} "
                        f"{self.price_changes['max_change']:.3f}% ({self.price_changes['max_change_direction']}) "
                        f"- {self.price_changes['changes_since_last_report']}éŠ˜æŸ„åˆ†ææ¸ˆã¿"
                    )

                # çµ±è¨ˆãƒªã‚»ãƒƒãƒˆ
                self.price_changes["max_change"] = 0.0
                self.price_changes["max_change_symbol"] = ""
                self.price_changes["max_change_direction"] = ""
                self.price_changes["last_report_time"] = now
                self.price_changes["changes_since_last_report"] = 0

        except Exception as e:
            logger.error(f"Error updating price change stats: {e}")

    async def _process_signal(self, signal):
        """å–å¼•ã‚·ã‚°ãƒŠãƒ«å‡¦ç†"""
        try:
            if (
                signal.signal_type == SignalType.LONG
                or signal.signal_type == SignalType.SHORT
            ):
                await self._process_entry_signal(signal)
            elif signal.signal_type == SignalType.CLOSE:
                await self._process_exit_signal(signal)

        except Exception as e:
            logger.error(f"Error processing signal: {e}")

    async def _process_entry_signal(self, signal):
        """ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚·ã‚°ãƒŠãƒ«å‡¦ç†"""
        symbol = signal.symbol
        side = signal.signal_type.value
        entry_price = signal.price

        logger.info(f"ğŸ”„ ENTRYå‡¦ç†é–‹å§‹: {symbol} {side} @ {entry_price:.6f}")

        # ãƒã‚¸ã‚·ãƒ§ãƒ³é–‹è¨­å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
        can_open, reason = self.position_manager.can_open_position(symbol)
        if not can_open:
            logger.warning(f"âŒ ENTRYæ‹’å¦: {symbol} {side} - ç†ç”±: {reason}")
            return

        # ãƒã‚¸ã‚·ãƒ§ãƒ³é–‹è¨­
        success, message, position = self.position_manager.open_position(
            symbol, side, entry_price, signal.timestamp
        )

        if success and position:
            logger.info(
                f"âœ… ENTRYæˆåŠŸ: {symbol} {side} @ {entry_price:.6f} "
                f"ã‚µã‚¤ã‚º: {position.size:.4f} ãƒ¬ãƒãƒ¬ãƒƒã‚¸: {position.max_leverage:.1f}x"
            )
            self.stats["trades_executed"] += 1

            # æˆ¦ç•¥ã«ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚’ç™»éŒ²
            self.strategy.add_position(
                symbol, side, entry_price, position.size, signal.timestamp
            )

            # å–å¼•è¨˜éŒ²
            trade_id = self.trade_record_manager.record_trade_open(position)
            logger.info(f"ğŸ“ å–å¼•è¨˜éŒ²ä½œæˆ: ID={trade_id}")

        else:
            logger.error(f"âŒ ENTRYå¤±æ•—: {symbol} {side} - {message}")

    async def _process_exit_signal(self, signal):
        """æ±ºæ¸ˆã‚·ã‚°ãƒŠãƒ«å‡¦ç†"""
        symbol = signal.symbol

        logger.info(
            f"ğŸ”„ EXITå‡¦ç†é–‹å§‹: {symbol} @ {signal.price:.6f} - ç†ç”±: {signal.reason}"
        )

        # ãƒã‚¸ã‚·ãƒ§ãƒ³æ±ºæ¸ˆ
        success, message, position = self.position_manager.close_position(
            symbol, signal.reason
        )

        if success and position:
            # PnLè¨ˆç®—
            realized_pnl = position.unrealized_pnl
            pnl_percent = (realized_pnl / (position.entry_price * position.size)) * 100

            logger.info(
                f"âœ… EXITæˆåŠŸ: {symbol} {position.side} @ {signal.price:.6f} "
                f"PnL: {realized_pnl:.2f} USDT ({pnl_percent:.2f}%)"
            )

            # æˆ¦ç•¥ã‹ã‚‰ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤
            tracker = self.strategy.remove_position(symbol)

            # å–å¼•è¨˜éŒ²ã‚’æ›´æ–°ï¼ˆç°¡ç•¥åŒ–ï¼‰
            logger.info(f"ğŸ“ å–å¼•å®Œäº†è¨˜éŒ²: {symbol} ç·åˆ©ç›Š {realized_pnl:.2f} USDT")

        else:
            logger.error(f"âŒ EXITå¤±æ•—: {symbol} - {message}")

    def _start_stats_timer(self):
        """çµ±è¨ˆè¡¨ç¤ºã‚¿ã‚¤ãƒãƒ¼é–‹å§‹"""

        def show_stats():
            if self.running:
                self._log_statistics()
                # æ¬¡ã®ã‚¿ã‚¤ãƒãƒ¼ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
                self.stats_timer = threading.Timer(60.0, show_stats)  # 1åˆ†é–“éš”
                self.stats_timer.daemon = True
                self.stats_timer.start()

        self.stats_timer = threading.Timer(60.0, show_stats)
        self.stats_timer.daemon = True
        self.stats_timer.start()

    def _log_statistics(self):
        """çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        try:
            # ã‚¢ãƒƒãƒ—ã‚¿ã‚¤ãƒ è¨ˆç®—
            uptime = (datetime.now() - self.stats["start_time"]).total_seconds()
            self.stats["uptime"] = uptime

            # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±è¨ˆå–å¾—
            data_stats = self.data_manager.get_stats() if self.data_manager else {}
            strategy_stats = self.strategy.get_stats() if self.strategy else {}
            position_stats = (
                self.position_manager.get_stats() if self.position_manager else {}
            )
            questdb_stats = (
                self.questdb_client.get_stats() if self.questdb_client else {}
            )
            symbol_stats = (
                self.symbol_mapper.get_mapping_stats() if self.symbol_mapper else {}
            )

            # ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªè¦ç´„
            portfolio = (
                self.position_manager.get_portfolio_summary()
                if self.position_manager
                else {}
            )

            logger.info("=== TRADE MINI STATISTICS ===")
            logger.info(f"Uptime: {uptime/3600:.2f} hours")
            logger.info(f"Ticks processed: {self.stats['ticks_processed']}")
            logger.info(f"Signals generated: {self.stats['signals_generated']}")
            logger.info(f"Trades executed: {self.stats['trades_executed']}")

            logger.info(f"Active symbols: {data_stats.get('active_symbols', 0)}")
            logger.info(f"Open positions: {position_stats.get('current_positions', 0)}")
            logger.info(
                f"Account balance: {portfolio.get('account_balance', 0):.2f} USDT"
            )
            logger.info(
                f"Total unrealized PnL: {portfolio.get('total_unrealized_pnl', 0):.2f} USDT"
            )

            logger.info(f"QuestDB ticks saved: {questdb_stats.get('ticks_saved', 0)}")
            logger.info(
                f"Tradeable symbols on Bybit: {symbol_stats.get('total_tradeable_symbols', 0)}"
            )
            logger.info("=============================")

        except Exception as e:
            logger.error(f"Error logging statistics: {e}")

    async def run(self):
        """ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œ"""
        logger.info("Starting Trade Mini...")

        try:
            # åˆæœŸåŒ–
            await self.initialize()

            # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
            self._setup_signal_handlers()

            self.running = True
            logger.info("Trade Mini is running. Press Ctrl+C to stop.")

            # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
            last_health_check = time.time()
            while self.running and not self.shutdown_event.is_set():
                try:
                    await asyncio.sleep(1.0)

                    # ğŸ©º ãƒ—ãƒ­ã‚»ã‚¹ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆ30ç§’æ¯ï¼‰
                    current_time = time.time()
                    if current_time - last_health_check >= 30.0:
                        self._check_multiprocess_health()
                        last_health_check = current_time

                    # å®šæœŸçš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                    if int(time.time()) % 300 == 0:  # 5åˆ†æ¯
                        self.position_manager.cleanup_closed_positions()

                except KeyboardInterrupt:
                    break
                except Exception as e:
                    logger.error(f"Error in main loop: {e}")
                    await asyncio.sleep(1.0)

            logger.info("Main loop ended")

        except Exception as e:
            logger.error(f"Critical error: {e}")
        finally:
            await self.shutdown()

    def _setup_signal_handlers(self):
        """ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š"""

        def signal_handler(signum, frame):
            logger.info(f"Received signal {signum}, initiating shutdown...")
            self.running = False
            self.shutdown_event.set()

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

    async def shutdown(self):
        """ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å‡¦ç†"""
        logger.info("Shutting down Trade Mini...")

        self.running = False
        self.shutdown_event.set()

        # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ãƒ¯ãƒ¼ã‚«ãƒ¼åœæ­¢
        if hasattr(self, "processing_active"):
            self.processing_active.value = False

        if hasattr(self, "data_processor") and self.data_processor:
            logger.info("Terminating multi-process data worker...")
            self.data_processor.terminate()
            self.data_processor.join(timeout=5)
            logger.info("Multi-process data worker terminated")

        try:
            # çµ±è¨ˆã‚¿ã‚¤ãƒãƒ¼åœæ­¢
            if self.stats_timer:
                self.stats_timer.cancel()

            # æœ€çµ‚çµ±è¨ˆè¡¨ç¤º
            self._log_statistics()

            # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
            if self.mexc_client:
                logger.info("Shutting down MEXC client...")
                await self.mexc_client.stop()

            if self.position_manager:
                logger.info("Shutting down position manager...")
                self.position_manager.shutdown()

            if self.questdb_client:
                logger.info("Shutting down QuestDB client...")
                self.questdb_client.shutdown()

            if self.data_manager:
                logger.info("Shutting down data manager...")
                self.data_manager.shutdown()

            logger.info("Trade Mini shutdown completed")

        except Exception as e:
            logger.error(f"Error during shutdown: {e}")

    def get_status(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®çŠ¶æ…‹ã‚’å–å¾—"""
        try:
            uptime = (datetime.now() - self.stats["start_time"]).total_seconds()

            return {
                "running": self.running,
                "uptime_hours": uptime / 3600,
                "stats": self.stats,
                "data_manager": (
                    self.data_manager.get_stats() if self.data_manager else {}
                ),
                "strategy": self.strategy.get_stats() if self.strategy else {},
                "positions": (
                    self.position_manager.get_stats() if self.position_manager else {}
                ),
                "questdb": (
                    self.questdb_client.get_stats() if self.questdb_client else {}
                ),
                "portfolio": (
                    self.position_manager.get_portfolio_summary()
                    if self.position_manager
                    else {}
                ),
            }
        except Exception as e:
            logger.error(f"Error getting status: {e}")
            return {"error": str(e)}


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹æ–¹æ³•ã‚’è¨­å®šï¼ˆDockerã‚³ãƒ³ãƒ†ãƒŠå¯¾å¿œï¼‰
        multiprocessing.set_start_method("fork", force=True)

        # Trade Mini ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
        app = TradeMini()

        # å®Ÿè¡Œ
        await app.run()

    except KeyboardInterrupt:
        print("Interrupted by user")
    except Exception as e:
        print(f"Fatal error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã§å®Ÿè¡Œ
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("Application interrupted")
    except Exception as e:
        print(f"Application failed: {e}")
        sys.exit(1)
